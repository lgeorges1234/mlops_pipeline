from sklearn.ensemble import StackingRegressor, RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd
import sys
import os
import pickle

from pathlib import Path

# Add the project root directory to Python path
project_dir = str(Path(__file__).resolve().parents[2])
sys.path.append(project_dir)

from src.config.model_settings import settings 


# Read the files using paths from settings

X_train_scaled = pd.read_csv(os.path.join(settings.processed_data_path, 'X_train_scaled.csv'))
X_test_scaled = pd.read_csv(os.path.join(settings.processed_data_path, 'X_test_scaled.csv'))
y_train = pd.read_csv(os.path.join(settings.processed_data_path, 'y_train.csv'))
y_test = pd.read_csv(os.path.join(settings.processed_data_path, 'y_test.csv'))
y_train = np.ravel(y_train)
y_test = np.ravel(y_test)

# 1. First, let's define our base models with their parameter grids
base_models = [
    ('rf', RandomForestRegressor(random_state=42), {
        'n_estimators': [100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    }),
    
    ('xgb', XGBRegressor(random_state=42), {
        'n_estimators': [100, 200],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.1],
        'subsample': [0.8, 1.0]
    }),
    
    ('svr', SVR(), {
        'C': [0.1, 1, 10],
        'kernel': ['rbf', 'linear'],
        'epsilon': [0.1, 0.2]
    })
]

# 2. Create and optimize each base model
optimized_models = []

for name, model, param_grid in base_models:
    print(f"\nOptimizing {name}...")
    
    # Create GridSearchCV object
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=5,
        scoring='neg_root_mean_squared_error',
        n_jobs=-1,
        verbose=1
    )
    
    # Fit GridSearchCV
    grid_search.fit(X_train_scaled, y_train)
    
    # Store optimized model
    optimized_models.append(
        (name, grid_search.best_estimator_)
    )
    
    print(f"Best parameters for {name}: {grid_search.best_params_}")
    print(f"Best RMSE: {-grid_search.best_score_:.4f}")

# 3. Create Stacking Ensemble
# Define final estimator
final_estimator = XGBRegressor(random_state=42)

# Create stacking ensemble
stack = StackingRegressor(
    estimators=optimized_models,
    final_estimator=final_estimator,
    cv=5
)

# 4. Define parameter grid for stacking ensemble
stack_param_grid = {
    'final_estimator__n_estimators': [100, 200],
    'final_estimator__learning_rate': [0.01, 0.1],
    'final_estimator__max_depth': [3, 5]
}

# 5. Optimize stacking ensemble
stack_grid_search = GridSearchCV(
    estimator=stack,
    param_grid=stack_param_grid,
    cv=5,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    verbose=1
)

stack_grid_search.fit(X_train_scaled, y_train)

# 6. Make predictions and evaluate
y_pred = stack_grid_search.predict(X_test_scaled)

# Calculate metrics
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\nFinal Stacking Ensemble Results:")
print(f"Best parameters: {stack_grid_search.best_params_}")
print(f"RMSE: {rmse:.4f}")
print(f"R2 Score: {r2:.4f}")

# 7. Compare with individual models
model_metrics = {}
print("\nModel Comparison:")
for name, model in optimized_models:
    y_pred = model.predict(X_test_scaled)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    model_metrics[name] = {'rmse': rmse, 'r2': r2}
    print(f"\n{name}:")
    print(f"RMSE: {rmse:.4f}")
    print(f"R2 Score: {r2:.4f}")

# Add stacking metrics
y_pred = stack_grid_search.predict(X_test_scaled)
stack_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
stack_r2 = r2_score(y_test, y_pred)
model_metrics['stacking'] = {'rmse': stack_rmse, 'r2': stack_r2}

# Find best model using both metrics
weights = {'rmse': 0.5, 'r2': 0.5}
best_score = float('-inf')
best_name = None

for name, metrics in model_metrics.items():
    normalized_rmse = 1 / (1 + metrics['rmse'])
    combined_score = weights['rmse'] * normalized_rmse + weights['r2'] * metrics['r2']
    if combined_score > best_score:
        best_score = combined_score
        best_name = name

# Save best model
models_dict = dict(optimized_models)
models_dict['stacking'] = stack_grid_search.best_estimator_
best_model = models_dict[best_name]

model_path = os.path.join(settings.models_path, 'best_model.pkl')
with open(model_path, 'wb') as f:
    pickle.dump(best_model, f)

print(f"\nBest model ({best_name}) saved:")
print(f"RMSE: {model_metrics[best_name]['rmse']:.4f}")
print(f"R2: {model_metrics[best_name]['r2']:.4f}")